{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4040c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook for training and testing a Player in Pokemon Showdown using Q-Learning with Function Approximation in Stochastic Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2693bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nComparative Table: https://prnt.sc/1ytqrzm\\n------\\nneptune\\nAction space: 4 moves + 5 switches\\npoke-env installed in C:\\\\Users\\\\-\\\\anaconda3\\\\envs\\\\poke_env\\\\lib\\\\site-packages\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Comparative Table: https://prnt.sc/1ytqrzm\n",
    "------\n",
    "neptune\n",
    "Action space: 4 moves + 5 switches\n",
    "poke-env installed in C:\\\\Users\\\\-\\\\anaconda3\\\\envs\\\\poke_env\\\\lib\\\\site-packages\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c309b2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb697f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "041dd8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import matplotlib\n",
    "import neptune.new as neptune\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import date\n",
    "from itertools import product\n",
    "from matplotlib import pyplot\n",
    "from poke_env.environment.abstract_battle import AbstractBattle\n",
    "from poke_env.player.battle_order import ForfeitBattleOrder\n",
    "from poke_env.player.player import Player\n",
    "# from poke_env.player.random_player import RandomPlayer\n",
    "from scipy.interpolate import griddata\n",
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.append('../')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a643b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.PlayerQLearning import Player as PlayerQLearning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e9d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global configs\n",
    "\n",
    "debug = True\n",
    "save_to_json_file = True\n",
    "use_validation = False\n",
    "use_neptune = False\n",
    "\n",
    "nest_asyncio.apply()\n",
    "np.random.seed(0)\n",
    "\n",
    "if use_neptune:\n",
    "    run = neptune.init(project='your_project',\n",
    "                       api_token='your_api_token',\n",
    "                       tags=[\"Q-learning FA\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b245274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of agent's team (Pokémon Showdown template)\n",
    "\n",
    "\n",
    "OUR_TEAM = \"\"\"\n",
    "Pikachu-Original (M) @ Light Ball  \n",
    "Ability: Static  \n",
    "EVs: 252 Atk / 4 SpD / 252 Spe  \n",
    "Jolly Nature  \n",
    "- Volt Tackle  \n",
    "- Nuzzle  \n",
    "- Iron Tail  \n",
    "- Knock Off  \n",
    "\n",
    "Charizard @ Life Orb  \n",
    "Ability: Solar Power  \n",
    "EVs: 252 SpA / 4 SpD / 252 Spe  \n",
    "Timid Nature  \n",
    "IVs: 0 Atk  \n",
    "- Flamethrower  \n",
    "- Dragon Pulse  \n",
    "- Roost  \n",
    "- Sunny Day  \n",
    "\n",
    "Blastoise @ White Herb  \n",
    "Ability: Torrent  \n",
    "EVs: 4 Atk / 252 SpA / 252 Spe  \n",
    "Mild Nature  \n",
    "- Scald  \n",
    "- Ice Beam  \n",
    "- Earthquake  \n",
    "- Shell Smash  \n",
    "\n",
    "Venusaur @ Black Sludge  \n",
    "Ability: Chlorophyll  \n",
    "EVs: 252 SpA / 4 SpD / 252 Spe  \n",
    "Modest Nature  \n",
    "IVs: 0 Atk  \n",
    "- Giga Drain  \n",
    "- Sludge Bomb  \n",
    "- Sleep Powder  \n",
    "- Leech Seed  \n",
    "\n",
    "Sirfetch’d @ Aguav Berry  \n",
    "Ability: Steadfast  \n",
    "EVs: 248 HP / 252 Atk / 8 SpD  \n",
    "Adamant Nature  \n",
    "- Close Combat  \n",
    "- Swords Dance  \n",
    "- Poison Jab  \n",
    "- Knock Off  \n",
    "\n",
    "Tauros (M) @ Assault Vest  \n",
    "Ability: Intimidate  \n",
    "EVs: 252 Atk / 4 SpD / 252 Spe  \n",
    "Jolly Nature  \n",
    "- Double-Edge  \n",
    "- Earthquake  \n",
    "- Megahorn  \n",
    "- Iron Head  \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "506122a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of opponent's team (Pokémon Showdown template)\n",
    "\n",
    "OP_TEAM = \"\"\"\n",
    "Eevee @ Eviolite  \n",
    "Ability: Adaptability  \n",
    "EVs: 252 HP / 252 Atk / 4 SpD  \n",
    "Adamant Nature  \n",
    "- Quick Attack  \n",
    "- Flail  \n",
    "- Facade  \n",
    "- Wish  \n",
    "\n",
    "Vaporeon @ Leftovers  \n",
    "Ability: Hydration  \n",
    "EVs: 252 HP / 252 Def / 4 SpA  \n",
    "Bold Nature  \n",
    "IVs: 0 Atk  \n",
    "- Scald  \n",
    "- Shadow Ball  \n",
    "- Toxic  \n",
    "- Wish  \n",
    "\n",
    "Sylveon @ Aguav Berry  \n",
    "Ability: Pixilate  \n",
    "EVs: 252 HP / 252 SpA / 4 SpD  \n",
    "Modest Nature  \n",
    "IVs: 0 Atk  \n",
    "- Hyper Voice  \n",
    "- Mystical Fire  \n",
    "- Psyshock  \n",
    "- Calm Mind  \n",
    "\n",
    "Jolteon @ Assault Vest  \n",
    "Ability: Quick Feet  \n",
    "EVs: 252 SpA / 4 SpD / 252 Spe  \n",
    "Timid Nature  \n",
    "IVs: 0 Atk  \n",
    "- Thunderbolt  \n",
    "- Hyper Voice  \n",
    "- Volt Switch  \n",
    "- Shadow Ball  \n",
    "\n",
    "Leafeon @ Life Orb  \n",
    "Ability: Chlorophyll  \n",
    "EVs: 252 Atk / 4 SpD / 252 Spe  \n",
    "Adamant Nature  \n",
    "- Leaf Blade  \n",
    "- Knock Off  \n",
    "- X-Scissor  \n",
    "- Swords Dance  \n",
    "\n",
    "Umbreon @ Iapapa Berry  \n",
    "Ability: Inner Focus  \n",
    "EVs: 252 HP / 4 Atk / 252 SpD  \n",
    "Careful Nature  \n",
    "- Foul Play  \n",
    "- Body Slam  \n",
    "- Toxic  \n",
    "- Wish  \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efad2678",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATE_COMPONENTS = 12\n",
    "# num of features = num of state components + action\n",
    "N_FEATURES = N_STATE_COMPONENTS + 1\n",
    "\n",
    "N_OUR_MOVE_ACTIONS = 4\n",
    "N_OUR_SWITCH_ACTIONS = 5\n",
    "N_OUR_ACTIONS = N_OUR_MOVE_ACTIONS + N_OUR_SWITCH_ACTIONS\n",
    "\n",
    "ALL_OUR_ACTIONS = np.array(range(0, N_OUR_ACTIONS))\n",
    "\n",
    "# Encoding Pokémon Name for ID\n",
    "\n",
    "NAME_TO_ID_DICT = {\n",
    "    \"pikachuoriginal\": 0,\n",
    "    \"charizard\": 1,\n",
    "    \"blastoise\": 2,\n",
    "    \"venusaur\": 3,\n",
    "    \"sirfetchd\": 4,\n",
    "    \"tauros\": 5,\n",
    "    \"eevee\": 6,\n",
    "    \"vaporeon\": 7,\n",
    "    \"sylveon\": 8,\n",
    "    \"jolteon\": 9,\n",
    "    \"leafeon\": 10,\n",
    "    \"umbreon\": 11\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ccdd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of MaxDamagePlayer\n",
    "\n",
    "class MaxDamagePlayer(Player):\n",
    "    def choose_move(self, battle):\n",
    "        if battle.available_moves:\n",
    "            best_move = max(battle.available_moves, key=lambda move: move.base_power)\n",
    "            return self.create_order(best_move)\n",
    "        else:\n",
    "            return self.choose_random_move(battle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d50c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of Q-Learning with Function Approximation Player\n",
    "\n",
    "class QLearningFAPlayer(PlayerQLearning):\n",
    "    def __init__(self, battle_format, team, n0, alpha0, gamma):\n",
    "        super().__init__(battle_format=battle_format, team=team)\n",
    "        self.N = defaultdict(lambda: np.zeros(N_OUR_ACTIONS))\n",
    "        self.w = np.random.rand(N_FEATURES)\n",
    "        self.n0 = n0\n",
    "        self.alpha0 = alpha0\n",
    "        self.gamma = gamma\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "\n",
    "    def choose_move(self, battle):\n",
    "        if self.state is not None:\n",
    "            # observe R, S'\n",
    "            reward = self.compute_reward(battle)\n",
    "            next_state = self.embed_battle(battle)\n",
    "            # Q-learning\n",
    "            self.N[str(self.state)][self.action] += 1\n",
    "            alpha = self.alpha0 / self.N[str(self.state)][self.action]\n",
    "            delta = \\\n",
    "                reward + self.gamma * self.max_q_approx(next_state, self.w) - self.q_approx(self.state, self.action, self.w)\n",
    "            self.w += alpha * delta * self.x(self.state, self.action)\n",
    "            # S <- S'\n",
    "            self.state = next_state\n",
    "        else:\n",
    "            # S first initialization\n",
    "            self.state = self.embed_battle(battle)\n",
    "\n",
    "        # Choose A from S using epsilon-greedy policy\n",
    "        self.action = self.pi(self.state, self.w)\n",
    "\n",
    "        # if the selected action is not possible, perform a random move instead\n",
    "        if self.action == -1:\n",
    "            return ForfeitBattleOrder()\n",
    "        elif self.action < 4 and self.action < len(battle.available_moves) and not battle.force_switch:\n",
    "            return self.create_order(battle.available_moves[self.action])\n",
    "        elif 0 <= self.action - 4 < len(battle.available_switches):\n",
    "            return self.create_order(battle.available_switches[self.action - 4])\n",
    "        else:\n",
    "            return self.choose_random_move(battle)\n",
    "\n",
    "    def _battle_finished_callback(self, battle):\n",
    "        if use_neptune:\n",
    "            run[f'N0: {self.n0} gamma: {self.gamma} win_acc'].log(self.n_won_battles / len(self._reward_buffer))\n",
    "\n",
    "    ''' Helper functions '''\n",
    "\n",
    "    # feature vector\n",
    "    @staticmethod\n",
    "    def x(state, action):\n",
    "        state = np.array(state).astype(float)\n",
    "        return np.append(state, action)\n",
    "\n",
    "    # q^(S, A, W)\n",
    "    def q_approx(self, state, action, w):\n",
    "        state = np.array(state).astype(float)\n",
    "        return np.dot(self.x(state, action), w)\n",
    "\n",
    "    # max(a, q^(S, a', W))\n",
    "    def max_q_approx(self, state, w):\n",
    "        state = np.array(state).astype(float)\n",
    "        return max(np.array([self.q_approx(state, action, w) for action in range(N_OUR_ACTIONS)]))\n",
    "\n",
    "    # epsilon-greedy policy\n",
    "    def pi(self, state, w):\n",
    "        epsilon = self.n0 / (self.n0 + np.sum(self.N[str(state)]))\n",
    "        # let's get the greedy action. Ties must be broken arbitrarily\n",
    "        q_approx = np.array([self.q_approx(state, action, w) for action in range(N_OUR_ACTIONS)])\n",
    "        greedy_action = np.random.choice(np.where(q_approx == q_approx.max())[0])\n",
    "        action_pick_probability = np.full(N_OUR_ACTIONS, epsilon / N_OUR_ACTIONS)\n",
    "        action_pick_probability[greedy_action] += 1 - epsilon\n",
    "        return np.random.choice(ALL_OUR_ACTIONS, p=action_pick_probability)\n",
    "\n",
    "    # the embed battle is our state\n",
    "    # 12 factors: our active mon, opponent's active mon, 4 moves base power, 4 moves multipliers, remaining mons\n",
    "    @staticmethod\n",
    "    def embed_battle(battle):\n",
    "        # -1 indicates that the move does not have a base power\n",
    "        # or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                    move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                )\n",
    "\n",
    "        # We count how many pokemons have not fainted in each team\n",
    "        n_fainted_mon_team = (\n",
    "            len([mon for mon in battle.team.values() if mon.fainted])\n",
    "        )\n",
    "        n_fainted_mon_opponent = (\n",
    "            len([mon for mon in battle.opponent_team.values() if mon.fainted])\n",
    "        )\n",
    "\n",
    "        state = list()\n",
    "        state.append(NAME_TO_ID_DICT[str(battle.active_pokemon).split(' ')[0]])\n",
    "        state.append(NAME_TO_ID_DICT[str(battle.opponent_active_pokemon).split(' ')[0]])\n",
    "        for move_base_power in moves_base_power:\n",
    "            state.append('{0:.2f}'.format(move_base_power))\n",
    "        for move_dmg_multiplier in moves_dmg_multiplier:\n",
    "            state.append('{0:.2f}'.format(move_dmg_multiplier))\n",
    "        state.append(n_fainted_mon_team)\n",
    "        state.append(n_fainted_mon_opponent)\n",
    "\n",
    "        return state\n",
    "\n",
    "    # Computing rewards\n",
    "    def reward_computing_helper(\n",
    "            self,\n",
    "            battle: AbstractBattle,\n",
    "            *,\n",
    "            fainted_value: float = 0.15,\n",
    "            hp_value: float = 0.15,\n",
    "            number_of_pokemons: int = 6,\n",
    "            starting_value: float = 0.0,\n",
    "            status_value: float = 0.15,\n",
    "            victory_value: float = 1.0\n",
    "    ) -> float:\n",
    "        # 1st compute\n",
    "        if battle not in self._reward_buffer:\n",
    "            self._reward_buffer[battle] = starting_value\n",
    "        current_value = 0\n",
    "\n",
    "        # Verify if pokemon have fainted or have status\n",
    "        for mon in battle.team.values():\n",
    "            current_value += mon.current_hp_fraction * hp_value\n",
    "            if mon.fainted:\n",
    "                current_value -= fainted_value\n",
    "            elif mon.status is not None:\n",
    "                current_value -= status_value\n",
    "\n",
    "        current_value += (number_of_pokemons - len(battle.team)) * hp_value\n",
    "\n",
    "        # Verify if opponent pokemon have fainted or have status\n",
    "        for mon in battle.opponent_team.values():\n",
    "            current_value -= mon.current_hp_fraction * hp_value\n",
    "            if mon.fainted:\n",
    "                current_value += fainted_value\n",
    "            elif mon.status is not None:\n",
    "                current_value += status_value\n",
    "\n",
    "        current_value -= (number_of_pokemons - len(battle.opponent_team)) * hp_value\n",
    "\n",
    "        # Verify if we won or lost\n",
    "        if battle.won:\n",
    "            current_value += victory_value\n",
    "        elif battle.lost:\n",
    "            current_value -= victory_value\n",
    "\n",
    "        # Value to return\n",
    "        to_return = current_value - self._reward_buffer[battle]\n",
    "        self._reward_buffer[battle] = current_value\n",
    "        if use_neptune:\n",
    "            run[f'N0: {self.n0} gamma: {self.gamma} reward_buffer'].log(current_value)\n",
    "        return to_return\n",
    "\n",
    "    # Calling reward_computing_helper\n",
    "    def compute_reward(self, battle) -> float:\n",
    "        return self.reward_computing_helper(battle, fainted_value=2, hp_value=1, victory_value=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64f13aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of Q-Learning with function approximation validation player\n",
    "\n",
    "class ValidationPlayer(PlayerQLearning):\n",
    "    def __init__(self, battle_format, team, w):\n",
    "        super().__init__(battle_format=battle_format, team=team)\n",
    "        self.w = w\n",
    "\n",
    "    def choose_move(self, battle):\n",
    "        state = self.embed_battle(battle)\n",
    "        # let's get the greedy action. Ties must be broken arbitrarily\n",
    "        q_approx = np.array([self.q_approx(state, action, self.w) for action in range(N_OUR_ACTIONS)])\n",
    "        action = np.random.choice(np.where(q_approx == q_approx.max())[0])\n",
    "\n",
    "        # if the selected action is not possible, perform a random move instead\n",
    "        if action == -1:\n",
    "            return ForfeitBattleOrder()\n",
    "        elif action < 4 and action < len(battle.available_moves) and not battle.force_switch:\n",
    "            return self.create_order(battle.available_moves[action])\n",
    "        elif 0 <= action - 4 < len(battle.available_switches):\n",
    "            return self.create_order(battle.available_switches[action - 4])\n",
    "        else:\n",
    "            return self.choose_random_move(battle)\n",
    "\n",
    "    def _battle_finished_callback(self, battle):\n",
    "        pass\n",
    "\n",
    "    ''' Helper functions '''\n",
    "\n",
    "    # feature vector\n",
    "    @staticmethod\n",
    "    def x(state, action):\n",
    "        state = np.array(state).astype(float)\n",
    "        return np.append(state, action)\n",
    "\n",
    "    # q^(S, A, W)\n",
    "    def q_approx(self, state, action, w):\n",
    "        state = np.array(state).astype(float)\n",
    "        return np.dot(self.x(state, action), w)\n",
    "\n",
    "    # the embed battle is our state\n",
    "    # 12 factors: our active mon, opponent's active mon, 4 moves base power, 4 moves multipliers, remaining mons\n",
    "    @staticmethod\n",
    "    def embed_battle(battle):\n",
    "        # -1 indicates that the move does not have a base power\n",
    "        # or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                    move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                )\n",
    "\n",
    "        # We count how many pokemons have not fainted in each team\n",
    "        fainted_mon_team = (\n",
    "            len([mon for mon in battle.team.values() if mon.fainted])\n",
    "        )\n",
    "        fainted_mon_opponent = (\n",
    "            len([mon for mon in battle.opponent_team.values() if mon.fainted])\n",
    "        )\n",
    "\n",
    "        state = list()\n",
    "        state.append(NAME_TO_ID_DICT[str(battle.active_pokemon).split(' ')[0]])\n",
    "        state.append(NAME_TO_ID_DICT[str(battle.opponent_active_pokemon).split(' ')[0]])\n",
    "        for move_base_power in moves_base_power:\n",
    "            state.append('{0:.2f}'.format(move_base_power))\n",
    "        for move_dmg_multiplier in moves_dmg_multiplier:\n",
    "            state.append('{0:.2f}'.format(move_dmg_multiplier))\n",
    "        state.append(fainted_mon_team)\n",
    "        state.append(fainted_mon_opponent)\n",
    "\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9320193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "\n",
    "# possible values for num_battles (number of episodes)\n",
    "n_battles_array = [10000]\n",
    "# exploration schedule from MC, i. e., epsilon(t) = N0 / (N0 + N(S(t)))\n",
    "n0_array = [0.0001, 0.001, 0.01]\n",
    "# possible values for alpha0 (initial learning rate)\n",
    "alpha0_array = [0.01]\n",
    "# possible values for gamma (discount factor)\n",
    "gamma_array = [0.75]\n",
    "\n",
    "\n",
    "list_of_params = [\n",
    "    {\n",
    "        'n_battles': n_battles,\n",
    "        'n0': n0,\n",
    "        'alpha0': alpha0,\n",
    "        'gamma': gamma\n",
    "    } for n_battles, n0, alpha0, gamma in product(n_battles_array, n0_array, alpha0_array, gamma_array)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "506f319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json helper functions\n",
    "\n",
    "def save_array_to_json(path_dir, filename, data):\n",
    "    if not os.path.exists(path_dir):\n",
    "        os.makedirs(path_dir)\n",
    "    full_filename = path_dir + \"/\" + filename\n",
    "    # write\n",
    "    with open(full_filename, \"w\") as file:\n",
    "        json.dump(data if isinstance(data, list) else data.tolist(), file)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "def save_dict_to_json(path_dir, filename, data, append=True):\n",
    "    if not os.path.exists(path_dir):\n",
    "        os.makedirs(path_dir)\n",
    "    full_filename = path_dir + \"/\" + filename\n",
    "    if os.path.exists(full_filename) and append:\n",
    "        with open(full_filename, \"r\") as file:\n",
    "            value_dict = json.load(file)\n",
    "            for key in data:\n",
    "                value_dict[key] = data[key] if isinstance(data[key], list) else data[key].tolist()\n",
    "            file.close()\n",
    "    else:\n",
    "        value_dict = dict()\n",
    "        for key in data:\n",
    "            value_dict[key] = data[key] if isinstance(data[key], list) else data[key].tolist()\n",
    "    # write\n",
    "    with open(full_filename, \"w\") as file:\n",
    "        json.dump(value_dict, file)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "def read_array_from_json(path_dir, filename):\n",
    "    full_filename = path_dir + \"/\" + filename\n",
    "    if not os.path.exists(full_filename):\n",
    "        return None\n",
    "    file = open(full_filename, \"r\")\n",
    "    data = json.load(file)\n",
    "    file.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_dict_from_json(path_dir, filename):\n",
    "    full_filename = path_dir + \"/\" + filename\n",
    "    if not os.path.exists(full_filename):\n",
    "        return None\n",
    "    file = open(full_filename, \"r\")\n",
    "    data = json.load(file)\n",
    "    file.close()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "684c1cbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m         save_dict_to_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Q_Learning_FA_statistics\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatistics.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, data)\n\u001b[1;32m     47\u001b[0m loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[0;32m---> 48\u001b[0m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo_battle_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nest_asyncio.py:84\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     82\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nest_asyncio.py:107\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m     heappop(scheduled)\n\u001b[1;32m    102\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    105\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 107\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    110\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m~/anaconda3/envs/poke_env/lib/python3.10/selectors.py:469\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    467\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main (let's battle!)\n",
    "\n",
    "# training\n",
    "async def do_battle_training():\n",
    "    for params in list_of_params:\n",
    "        start = time.time()\n",
    "        params['player'] = QLearningFAPlayer(battle_format=\"gen8ou\", team=OUR_TEAM, n0=params['n0'], alpha0=params['alpha0'], gamma=params['gamma'])\n",
    "        params['opponent'] = MaxDamagePlayer(battle_format=\"gen8ou\", team=OP_TEAM)\n",
    "        await params['player'].battle_against(opponent=params['opponent'], n_battles=params['n_battles'])\n",
    "        if debug:\n",
    "            print(\"training: num battles (episodes)=%d, N0=%.4f, alpha0=%.2f, gamma=%.2f, wins=%d, winning %%=%.2f, total time=%s sec\" %\n",
    "                  (\n",
    "                      params['n_battles'],\n",
    "                      round(params['n0'], 4),\n",
    "                      round(params['alpha0'], 2),\n",
    "                      round(params['gamma'], 2),\n",
    "                      params['player'].n_won_battles,\n",
    "                      round((params['player'].n_won_battles / params['n_battles']) * 100, 2),\n",
    "                      round(time.time() - start, 2)\n",
    "                  ))\n",
    "\n",
    "        # save w to json file\n",
    "        if save_to_json_file:\n",
    "            today_s = str(date.today())\n",
    "            n_battle_s = str(params['n_battles'])\n",
    "            n0_s = str(round(params['n0'], 4))\n",
    "            alpha0_s = str(round(params['alpha0'], 2))\n",
    "            gamma_s = str(round(params['gamma'], 2))\n",
    "            winning_percentage_s = str(round((params['player'].n_won_battles / params['n_battles']) * 100, 2))\n",
    "            filename = \"W_\" + today_s + \"_\" + n_battle_s + \"_\" + n0_s + \"_\" + alpha0_s + \"_\" + gamma_s + \"_\" + winning_percentage_s + \".json\"\n",
    "            save_array_to_json(\"./Q_Learning_FA_w\", filename, params['player'].w)\n",
    "\n",
    "        # statistics: key: \"n_battles, n0, alpha0, gamma\", values: list of win or lose\n",
    "        key = str(params['n_battles']) + \"_\" + str(round(params['n0'], 4)) + \"_\" + str(round(params['alpha0'], 2)) + \"_\" + str(round(params['gamma'], 2))\n",
    "        winning_status = list()\n",
    "        for battle in params['player']._battles.values():\n",
    "            if battle.won:\n",
    "                winning_status.append(True)\n",
    "            else:\n",
    "                winning_status.append(False)\n",
    "        # save statistics json file (append)\n",
    "        data = dict()\n",
    "        data[key] = winning_status\n",
    "        save_dict_to_json(\"./Q_Learning_FA_statistics\", \"statistics.json\", data)\n",
    "\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(loop.create_task(do_battle_training()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation - vs maxPlayer\n",
    "\n",
    "async def do_battle_validation(path_dir):\n",
    "    # read from json\n",
    "    for filename in os.listdir(path_dir):\n",
    "        # learned feature vector\n",
    "        w = np.array(read_array_from_json(path_dir, filename))\n",
    "        # params: n_battles, n0, gamma\n",
    "        params = filename.split(\"_\")\n",
    "        n_battles = int(params[2])\n",
    "        n0 = float(params[3])\n",
    "        alpha0 = float(params[4])\n",
    "        gamma = float(params[5])\n",
    "\n",
    "        # validation (play 1/3 of the battles using Q-learned table)\n",
    "        start = time.time()\n",
    "        validation_player = ValidationPlayer(battle_format=\"gen8ou\", team=OUR_TEAM, w=w)\n",
    "        opponent = MaxDamagePlayer(battle_format=\"gen8ou\", team=OP_TEAM)\n",
    "        n_battles_validation = int(n_battles / 3)\n",
    "        await validation_player.battle_against(opponent=opponent, n_battles=n_battles_validation)\n",
    "        print(\"validation: num battles (episodes)=%d, N0=%.4f, alpha0=%.2f, gamma=%.2f, wins=%d, winning %%=%.2f, total time=%s sec\" %\n",
    "              (\n",
    "                  n_battles_validation,\n",
    "                  n0,\n",
    "                  alpha0,\n",
    "                  gamma,\n",
    "                  validation_player.n_won_battles,\n",
    "                  round((validation_player.n_won_battles / n_battles_validation) * 100, 2),\n",
    "                  round(time.time() - start, 2)\n",
    "              ))\n",
    "\n",
    "\n",
    "if use_validation:\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(loop.create_task(do_battle_validation(\"./Q_Learning_FA_w\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ac540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation - vs randomPlayer\n",
    "\n",
    "async def do_battle_validation(path_dir):\n",
    "    # read from json\n",
    "    for filename in os.listdir(path_dir):\n",
    "        # learned feature vector\n",
    "        w = np.array(read_array_from_json(path_dir, filename))\n",
    "        # params: n_battles, n0, gamma\n",
    "        params = filename.split(\"_\")\n",
    "        n_battles = int(params[2])\n",
    "        n0 = float(params[3])\n",
    "        alpha0 = float(params[4])\n",
    "        gamma = float(params[5])\n",
    "\n",
    "        # validation (play 1/3 of the battles using Q-learned table)\n",
    "        start = time.time()\n",
    "        validation_player = ValidationPlayer(battle_format=\"gen8ou\", team=OUR_TEAM, w=w)\n",
    "        opponent = MaxDamagePlayer(battle_format=\"gen8ou\", team=OP_TEAM)\n",
    "        n_battles_validation = int(n_battles / 3)\n",
    "        await validation_player.battle_against(opponent=opponent, n_battles=n_battles_validation)\n",
    "        print(\"validation: num battles (episodes)=%d, N0=%.4f, alpha0=%.2f, gamma=%.2f, wins=%d, winning %%=%.2f, total time=%s sec\" %\n",
    "              (\n",
    "                  n_battles_validation,\n",
    "                  n0,\n",
    "                  alpha0,\n",
    "                  gamma,\n",
    "                  validation_player.n_won_battles,\n",
    "                  round((validation_player.n_won_battles / n_battles_validation) * 100, 2),\n",
    "                  round(time.time() - start, 2)\n",
    "              ))\n",
    "\n",
    "\n",
    "if use_validation:\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(loop.create_task(do_battle_validation(\"./Q_Learning_FA_w\")))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
